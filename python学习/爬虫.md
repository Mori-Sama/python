# 爬虫

## requests模块使用

~~~python
import requests
# 指定url
url = 'https://www.x23qb.com/'
# 发起get请求，返回的是一个包含响应数据的响应对象
res = requests.get(url)
# 以字符串的形式展示响应数据
obj = res.text
~~~

~~~python
# 实现一个简易的网页采集器，让url携带的参数动态化
# url = 'https://www.baidu.com/s?wd=周杰伦'，分析url后发现参数是 wd=周杰伦 于是：
# url中的参数是?号后面的，每一个参数用&隔开，可在query string parameters中找到
# 实现参数动态化
url = 'https://www.baidu.com/s'
# 输入关键字
wd = input('key')
# 封装成字典
params = {
    'wd':wd
}
# 在请求中需要将请求参数对应的字典作用到params这个get方法的参数中
res = requests.get(url,params=params)
# 在url中?后面的都是参数，可以封装到字典中作为参数传入
# 如果出现乱码可以对响应对象先编码
res.encoding = 'utf-8'
~~~

**UA检测**：门户网站通过检测请求载体的身份标识判定该请求是否为浏览器发起

~~~python
# UA伪装
# headers字典封装的是关于请求头的请求数据
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'
}
# 把参数传入
res = requests.get(url,params=params,headers=headers)
~~~

## **ajax请求**：

~~~python
# 动态加载的页面数据：通过另一个单独的请求请求到的数据。
# f12打开抓包工具，network中的xhr中含有ajax请求，找到ajax请求的请求网址，对其发起请求
# 如果请求对象的响应数据response是json数据格式的，可以用以下方法：
	request对象.json() # 返回的是序列化好的实列对象
# 将请求参数封装成字典
# 如果ajax请求发起的时post请求，则字典封装的参数为data，并且由request.post(url,headers=headers,data=data)发起请求
# ctrl+f 打开f12中的搜索，定位动态加载的数据包，从数据包中提取url和请求参数
~~~

## **图片爬取**

~~~python
# 找到图片对应的url，发起请求
# res = response.content 返回bytes类型的数据
# 保存写入数据
with open('xx.jpg',mode='w') as f:
    f.write(res)
# 视频音频等也是一样的操作
~~~

## **数据解析**

~~~python
# bs4解析
from bs4 import BeautifulSoup
# 实例化一个BeautifulSoup对象，需要将即将被解析的页面源码数据加载到该对象中
# 调用BeautifulSoup对象中的相关方法和属性进行标签定位和数据提取

# 实例化
# soup = BeautifulSoup(page_text,'lxml') lxml是解析方式，page_text是页面源码数据

# 定位标签的操作
# 标签定位
soup.tagName:定位到第一个出现的tagName标签
    soup.div:定位到第一个出现的div标签
# 属性定位(find只能获取第一个发现的，返回一个标签字符串，find_all获取所有满足条件的，返回的是一个列表)
soup.find('div',class_='song'):定位div标签里面类为song的标签，注意class_需要加下划线，并且只有class需要加
soup.find('div',id='feng'):定位div标签里面id为feng的标签
soup.find_all('div',class_='song'):定位所有的div标签里面类为song的标签
# 选择器定位soup.select('选择器')
	# 标签名
    soup.select('a') # 输出所有的a标签
    # 类选择器
	soup.select('.song') # 输出所有类为song的标签
    # id选择器
    soup.select('#link') # 输出所有id为link的标签
    # 属性选择器
    	# 选择a标签，其属性中存在myname的所有标签
        soup.select("a[myname]")
        # 选择a标签，其属性href=http://example.com/lacie的所有标签
        soup.select("a[href='http://example.com/lacie']")
        # 选择a标签，其href属性以http开头
        soup.select('a[href^="http"]')
        # 选择a标签，其href属性以lacie结尾
        soup.select('a[href$="lacie"]')
        # 选择a标签，其href属性包含.com
        soup.select('a[href*=".com"]')
    # 组合查找
    soup.select('p #link') # 输出p标签下所有id为link的标签
    # 子标签查找
    soup.select('div > p') # 输出所有div标签下一层级的p标签
# 注意子标签查找和组合查找的不同：(div p)这种用空格隔开的表示div标签下所有的p标签不论是第几层级的，而(div > p)这种只表示div标签下一层级的p标签，如果p标签中还嵌套着p，那么嵌套的p是找不到的

# 获取文本
# string获取标签直系文本内容
# text获取标签内所有的文本内容

# 取属性
# 定位到的标签['属性']/tagName[attrName]
~~~

~~~python
# xpath解析
# 实例化一个etree的对象，然后将即将被解析的页面源码加载到该对象中
# 使用etree对象中的xpath方法结合着不同形式的xpath表达式实现标签定位，返回的是对象列表
from lxml import etree
tree = etree.parse('test.html') # 本地的html文件
tree = etree.HTML(page_text) # 通过网络获取的网页源码
# 绝对路径
tree.xpath('/html/body/div/p') # 逐层寻找，最左侧的单斜杠表示一层一层的找，并且必须从html出发
# 相对路径
tree.xpath('//p') # 从任意位置寻找，只要有p标签，则全部获取
tree.xpath('/html/body/div//p') # 输出body的div标签下的所有p标签
# 属性定位：//tagName[@attrName="value"]
# 索引定位：//tagName[index],注意索引是从1开始

# 取文本：/text() 取直系文本内容 //text() 取所有的文本内容
# 取属性：/@attrName

# xpath解析出的对象可以继续使用xpath进行局部解析，此时xpath的语法开头需要用 .来表明为局部解析
div.xpath('./div[1]/a[2]')

# 针对中文乱码的通用方法
源数据.encode('iso-8859-1').decode('gbk')

# 获取xpath
# f12打开抓包工具，选择对应标签，右键选择copy

# xpath表达式可以用|管道符连接，如果所有的表达式都生效，那么都执行
div.xpath('./div[1]/a[2] | ./div[2]/a[1]')

# tbody不能出现在xpath表达式中，否则xpath表达式会失效
~~~

## **代理操作**

~~~python
# 获取代理ip的网站： www.goubanjia.com 快代理 http://http.zhiliandaili.cn/Shop-long.html
# 不管是get还是post请求，都有一个proxies参数
# proxies参数对应一个字典
proxies = {
    'http':'175.43.35.46:9999'
}
# 构建代理池
import random
proxies_list = [
    {'https':'113.194.28.61:9999'},
    {'http':'123.169.125.208:9999'},
    {'http':'115.221.240.115:9999'}
]
requests.get(url,headers=headers,proxies=random.choice(proxies_list))
~~~

## **cookie处理**

~~~python
# 手动处理，将cookie封装到headers中
# 自动处理：创建session对象，该对象可以像requests一样发送请求，如果产生了cookie，则cookie会被自动存储在session中
session = requests.Session()
session.get(url,headers=headers) # 访问获取cookie的网址,并存储cookie
session.get(ajax_url,headers=headers) # 访问需要该cookie的网址，获取数据
~~~

## **模拟登录**

~~~python
# 验证码识别网址：超级鹰
# 点击登录，f12抓取发送的数据包，解析url和参数、cookie
# 找到需要的网址，对其发送请求，将参数封装成字典传入
~~~

## **动态变化的请求参数**

~~~python
# 一般来说动态变化的请求参数一般会被隐藏在前台页面源码中
# f12查看前台页面源码，ctrl+f搜索参数名称
~~~

## **单线程多任务异步协程**

~~~python
# 协程
	# 在函数定义的时候，如果使用了async修饰的话，则该函数调用后会返回一个协		 程对象，并且函数体内的代码不会被立即执行
# 任务对象
	# 任务对象就是对协程对象的进一步封装
    # 任务对象必须要注册到时间循环对象中
    # 给任务对象绑定回调：爬虫的数据解析中
# 事件循环
	# 当做是一个容器，容器中必须存放任务对象
    # 当启动事件循环对象后，则事件循环对象会对其内部存储的任务对象进行异步的	   执行
# aiohttp：支持异步请求的模块，参数和requests相同

import asyncio
import aiohttp
from lxml import etree
urls = [
    'www.baidu.com',
    'www.google.com',
]
tasks = []
# 在特殊函数（用async修饰的def）内部的实现中不可以出现不支持异步的模块代码
async def get_requests(url):
    # 手动对阻塞操作添加await修饰词
    async with aiohttp.ClientSession() as s:
        async with await s.get(url) as response:
            page_text = await response.text()
    return page_text
# 回调函数，作用是对数据进行解析、处理
def callback(task):
    # task.result() 用来接收特殊函数的返回值即任务对象的返回值
    page_text = task.result()
    tree = etree.HTML(page_text)
    parse_data = tree.xpath('...')
for url in urls:
    # 封装任务对象
    c = get_requests(url)
    task = asyncio.ensure_future(c)
    # 给任务对象绑定回调函数
    task.add_done_callback(callback)
    tasks.append(task)
# 创建事件循环
loop = asyncio.get_event_loop()
# 注册事件，挂起操作需要手动执行asyncio.wait(tasks)
loop.run_until_complete(asyncio.wait(tasks))

# 线程池
from multiprocessing.dummy import Pool
pool = Pool(3) # 开启一个容量为3的线程池
pool.map(func,alist) # func为函数，alist为一个列表（也可以是一个可迭代对象）

~~~

# **selenium模块**

~~~python
from selenium import webdriver
# 在实现标签定位的时候，如果发现定位的标签是存在于iframe标签之中的，则在定位时必须执行一个固定操作：bro.switch_to.frame('iframe标签的id')
# 实例化某一款浏览器对象
browser = webdriver.Chrome(executable_path='path') # 浏览器驱动程序可以放在虚拟环境的script文件夹内，后面的executable_path参数可以不写，executable_path表示的是浏览器驱动程序的路径
# 对网址发起请求
browser.get(url)

# 定位单个节点，返回的是一个webelement对象
# 通过id值匹配
res = browser.find_element_by_id(' ')
# 通过name值匹配
res = browser.find_element_by_name(' ')
# 通过class属性值匹配
res = browser.find_element_by_class_name('')
# 通过css选择器匹配
res = browser.find_element_by_css_selector('')
# 通过xpath语法匹配
res = browser.find_element_by_xpath(' ')
res = find_element_by_xpath("//input[@id='kw' and @name='wd']") # 可以用and连接
# 通过标签名匹配
res = browser.find_element_by_tag_name("")
# 通过文本链接匹配
res = browser.find_element_by_link_text("新闻")
# 针对一些比较长的文本链接，取其中的一小部分文本值进行匹配
res = browser.find_element_by_partial_link_text("贴")
# 如果要定位多个节点只需要把以上代码的element改为elements，返回的是一个包含着element对象的列表

# element对象的操作
clear # 清除元素的内容
send_keys() # 模拟按键输入，如果需要输入中文，防止编码错误使用send_keys(u"中文用户名")。
click() # 点击元素
submit() # 提交表单
size() # 获取元素的尺寸，返回一个字典：{'x': 000, 'y': 000}
text() # 获取元素的文本
get_attribute(name) # 获取属性值
location() # 获取元素坐标，先找到要获取的元素，再调用该方法，返回一个字典：{'height': 000, 'width': 000}
is_displayed() # 设置该元素是否可见
is_enabled() # 判断元素是否被使用
is_selected() # 判断元素是否被选中
tag_name() # 返回元素的tagName

# 浏览器操作
title # 返回页面标题
page_source # 返回页面源码
current_url # 获取当前页面的URL
refresh() # 刷新浏览器
set_window_size() # 设置浏览器的大小
set_window_position() # 设置浏览器的坐标
set_window_rect() # 设置浏览器的矢量
maximize_window() # 最大化窗口
minimize_window() # 最小化窗口
close() # 关闭页面
quit() # 关闭窗口

# 鼠标事件
from selenium.webdriver import ActionChains
ActonChains(driver)  # 构造ActionChains对象
click(on_element) # 左击
context_click(on_element)  # 右键，另存为等行为
double_click(on_element)  # 左键双击，地图web可实现放大功能
drag_and_drop(source,target)  # 左键拖动，源元素按下左键移动至目标元素释放
drag_and_drop_by_offset(source, xoffset, yoffset)  # 拖拽到某个坐标然后松开
key_down(value, element=None)  # 按下某个键盘上的键
key_up(value, element=None)  # 松开某个键
move_by_offset(xoffset, yoffset)  # 鼠标从当前位置移动到某个坐标
move_to_element(on_element)  # 鼠标悬停
move_to_element_with_offset(to_element, xoffset, yoffset)  # 移动到距某个元素（左上角坐标）多少距离的位置
click_and_hold(on_element)  #左键点击不松开
perform()  # 在通过调用该函数执行ActionChains中存储行为

# 键盘事件
from selenium.webdriver.common.keys import Keys
send_keys(Keys.ENTER) # 按下回车键
send_keys(Keys.TAB) # 按下Tab制表键
send_keys(Keys.SPACE) # 按下空格键space
send_keys(Kyes.ESCAPE) # 按下回退键Esc
send_keys(Keys.BACK_SPACE) # 按下删除键BackSpace
send_keys(Keys.SHIFT) # 按下shift键
send_keys(Keys.CONTROL) # 按下Ctrl键
send_keys(Keys.ARROW_DOWN) # 按下鼠标光标向下按键
send_keys(Keys.CONTROL,'a') # 组合键全选Ctrl+A
send_keys(Keys.CONTROL,'c') # 组合键复制Ctrl+C
send_keys(Keys.CONTROL,'x') # 组合键剪切Ctrl+X
send_keys(Keys.F1…Fn) # 键盘 F1…Fn
~~~

~~~python
# 设置元素等待

# 隐式等待
driver.implicitly_wait(10) #implicitly_wait() 默认参数的单位为秒，本例中设置等待时长为10秒。首先这10秒并非一个固定的等待时间，它并不影响脚本的执行速度。其次，它并不针对页面上的某一元素进行等待。当脚本执行到某个元素定位时，如果元素可以定位，则继续执行；如果元素定位不到，则它将以轮询的方式不断地判断元素是否被定位到。假设在第6秒定位到了元素则继续执行，若直到超出设置时长（10秒）还没有定位到元素，则抛出异常。

# 显示等待
from selenium import webdriver 
from selenium.webdriver.support.wait import WebDriverWait 
from selenium.webdriver.support import expected_conditions as EC 
from selenium.webdriver.common.by import By

locator=(By.XPATH,'xxxxxxx')
d = webdriver.Chorme()
d.get('http://www.sina.com')
WebDriverWait(d,10,1).unitl(EC.presence_of_element_located(locator))
# 这里表示等待10s，每隔1s去检查一次元素是否出现，出现了就执行下一步，直到10s结束后还没有出现就会抛出异常。
WebDriverWait(driver,timeout,poll_frequency=0.5,ignored_exceptions=None)
	'''
    driver ：浏览器驱动。
    timeout ：最长超时时间，默认以秒为单位。
    poll_frequency ：检测的间隔（步长）时间，默认为0.5S。
    ignored_exceptions ：超时后的异常信息，默认情况下抛NoSuchElementException异常。
	'''

# title_is 标题是某内容
WebDriverWait(driver,10).until(EC.title_is(u"百度一下，你就知道"))
# 返回bool

# title_contains 标题包含某内容
WebDriverWait(driver,10).until(EC.title_contains(u"百度一下"))
# 返回bool

# presence_of_element_located 元素加载出，传入定位元组，如(By.ID, 'p')
WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID,'kw')))
# 判断某个元素是否被加到了dom树里，并不代表该元素一定可见，如果定位到就返回WebElement

# visibility_of_element_located 元素可见，传入定位元组
WebDriverWait(driver,10).until(EC.visibility_of_element_located((By.ID,'su')))
# 判断某个元素是否被添加到了dom里并且可见，可见代表元素可显示且宽和高都大于0

# visibility_of 可见，传入元素对象
WebDriverWait(driver,10).until(EC.visibility_of(driver.find_element(by=By.ID,value='kw')))
# 判断元素是否可见，如果可见就返回这个元素

# presence_of_all_elements_located 所有元素加载出
WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'.mnav')))
# 判断是否至少有1个元素存在于dom树中，如果定位到就返回列表

# text_to_be_present_in_element 某个元素文本包含某文字
WebDriverWait(driver,10).until(EC.text_to_be_present_in_element((By.XPATH,"//*[@id='u1']/a[8]"),u'设置'))
# 判断指定的元素中是否包含了预期的字符串，返回布尔值

# text_to_be_present_in_element_value 某个元素值包含某文字
WebDriverWait(driver,10).until(EC.text_to_be_present_in_element_value((By.CSS_SELECTOR,'#su'),u'百度一下'))
# 判断指定元素的属性值中是否包含了预期的字符串，返回布尔值

# frame_to_be_available_and_switch_to_it frame加载并切换
WebDriverWait(driver,10).until(EC.frame_to_be_available_and_switch_to_it(locator))
# 判断该frame是否可以switch进去，如果可以的话，返回True并且switch进去，否则返回False,注意这里并没有一个frame可以切换进去

# invisibility_of_element_located 元素不可见
WebDriverWait(driver,10).until(EC.invisibility_of_element_located((By.CSS_SELECTOR,'#swfEveryCookieWrap')))
'''判断某个元素在是否存在于dom或不可见,如果可见返回False,不可见返回这个元素'''

# element_to_be_clickable 元素可点击
WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.XPATH,"//*[@id='u1']/a[8]"))).click()
'''判断某个元素中是否可见并且是enable的，代表可点击'''

# staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新
WebDriverWait(driver,10).until(EC.staleness_of(driver.find_element(By.ID,'su')))
'''等待某个元素从dom树中移除'''

# element_to_be_selected 元素可选择，传元素对象
WebDriverWait(driver,10).until(EC.element_to_be_selected(driver.find_element(By.XPATH,"//*[@id='nr']/option[1]")))
'''判断某个元素是否被选中了,一般用在下拉列表'''

# element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False
WebDriverWait(driver,10).until(EC.element_selection_state_to_be(driver.find_element(By.XPATH,"//*[@id='nr']/option[1]"),True))
'''判断某个元素的选中状态是否符合预期'''

# element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False
WebDriverWait(driver,10).until(EC.element_located_selection_state_to_be((By.XPATH,"//*[@id='nr']/option[1]"),True))
'''判断某个元素的选中状态是否符合预期'''

# alert_is_present 是否出现Alert
instance = WebDriverWait(driver,10).until(EC.alert_is_present())
'''判断页面上是否存在alert,如果有就切换到alert并返回alert的内容'''
~~~

~~~python
# 窗口截图
#1.截取当前窗⼝，并指定截图图⽚的保存位置
driver.get_screenshot_as_file("D:\\baidu_img.jpg")
~~~

~~~python
# selenium谷歌浏览器规避检测
from selenuim import webdriver
driver = webdriver.Chrome()
driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
  "source": """
    Object.defineProperty(navigator, 'webdriver', {
      get: () => undefined
    })
  """
})
~~~

# scrapy模块

## 创建工程

~~~python
# 终端指令
scrapy startproject 项目名
# 创建一个爬虫文件
cd 项目名
scrapy genspider 文件名 起始url
# 执行
scrapy crawl 文件名
~~~

## 爬虫文件

~~~python
import scrapy


class ManhuafenSpider(scrapy.Spider):
    # 爬虫文件名称：爬虫源文件的唯一标识
    name = 'manhuafen'
    # 允许的域名，一般注释掉，作用是限制爬取的网址
    allowed_domains = ['www.xxx.com']
    # 起始url列表：列表中的元素会自动的进行请求发送
    start_urls = ['http://www.xxx.com/']
    # 解析数据
    def parse(self, response):
        pass
~~~

## 配置文件

~~~python
ROBOTSTXT_OBEY = False # 设置不遵循robot协议
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36' # 配置UA请求头
LOG_LEVEL = 'ERROR' # 设置日志等级
LOG_FILE = 'log.txt' # 设置日志输出文件
~~~

## response的操作

~~~python
response.xpath() # xpath解析
# xpath解析返回列表中的列表元素是Selector对象
# 我们要解析的数据存在这个对象中
# 需要 .extract_first() 操作取出其中的字符串
# 如果有多个则用 .extract() 返回一个列表
~~~

## 持久化存储

~~~python
# 存储到磁盘文件上
scrapy crawl 文件名 -o 存储文件路径（格式是.csv）
# 这个存储的是parse方法的返回值，需要在parse方法下面写上return

# 基于管道pipelines.py
# 书写items文件下的类
class ManhuaItem(scrapy.Item):
    content = scrapy.Field() # Field是一个万能数据类型
	name = scrapy.Field()
# 导入items文件中的类
from manhua.items import ManhuaItem
# 实例化对象
item = ManhuaItem()
# 将解析的数据存储到item类型的对象中
item['content'] = 解析出的对象
item['name'] = 解析出的对象

# 将item对象提交给管道
yield item

# 在管道类中process_item负责接收item
class ManhuaPipeline:
    fp = None
    def open_spider(self,spider):
        # 重写父类，这个方法只会在爬虫开始的时候执行一次
        self.fp = open('test.txt','w',encoding='utf-8')
    def process_item(self, item, spider):
        # 取出item中的对象
        content = item['content']
        # 存储
        self.fp.write(content)
        return item
    def close_spider(self,spider):
        # 重写父类，这个方法只会在爬虫结束时执行一次
        self.fp.close()

# 在settings文件中设置
ITEM_PIPELINES = {
   'manhua.pipelines.ManhuaPipeline': 300,
} # 300表示的是优先级，值越小优先级越高

# 在pipelines文件中可以写多个管道类，一个管道类对应的是一种持久化存储的方式
# 爬虫的item会提交给优先级最高的管道类
# process_item方法中的return item就表示，把item传递给下一个执行的管道类
~~~

## 手动请求发送+五大组件

~~~python
# 手动请求发送(get请求)
yield scrapy.Request(url,callback)
# 手动请求发送(post请求)
yield scrapy.FormRequest(url,formdata,callback)
# 回调函数仿照parse写，或者就用parse
~~~

![scrapy](C:\Users\Zhao\Desktop\图\scrapy.jpg)

~~~python
# 引擎（Scrapy）
用来处理整个系统的数据流，触发事务
# 调度器（Scheduler）
用来接收引擎发过来的请求，压入队列中，并在引擎再次请求时返回，可以当作一个url的优先队列，由它来决定下一个要抓取的网址时什么，同时去除重复的网址
# 下载器（Downloader）
用于下载网页内容，并将网页内容返回给爬虫（scrapy下载器时建立在twisted这个高效异步模型上的）
# 爬虫（Scrapy）
用于从特定的网页中提取自己需要的信息
# 项目管道（Pipeline）
负责处理数据
~~~

## 请求传参

~~~python
# 作用：实现深度爬取
# 使用场景：使用scrapy爬取的数据没有存在同一张页面中

# 在爬虫文件中item也可以通过参数的形式传递给callback
yield scrapy.Request(url,callback,meta={'item':item})

def callback(self,response):
    item = response.meta['item']
    yield item
# meta参数是一个字典，封装了键值，可以传递给callback
~~~

## 提升爬取效率

~~~python
1.增加并发：
    默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。
 
2.降低日志级别：
    在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’
 
3.禁止cookie：
    如果不是真的需要cookie，则在scrapy爬取数据时可以进制cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False
 
4.禁止重试：
    对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False
 
5.减少下载超时：
    如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s
~~~

## scrapy中间件

~~~python
# 爬虫中间件，位于爬虫和引擎之间（一般不用）
# 作用：拦截请求和item

# 下载中间件，位于引擎和下载器之间
# 作用：批量拦截请求和响应
# middlewares.py
class ManhuaDownloaderMiddleware:

    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s
	
    # 拦截请求，spider参数是爬虫类的实例化对象，可以实现爬虫类与中间件的交互
    def process_request(self, request, spider):
        # 设置请求头
        request.header['User-Agent'] = random.choice(user_agent_list)
        # 设置代理ip
        request.meta['proxy'] = 'http://111.29.3.194:8080'

        return None
	# 拦截响应
    def process_response(self, request, response, spider):
        new_response = HtmlResponse(url=request.url,body=txt,encoding='utf-8',request=request)
        # 参数url就表示请求的url，body表示经过中间件处理后的获取的页面内容
        return new_response

    # 拦截发生异常的请求
    def process_exception(self, request, exception, spider):
		# 修正request
        return request # 重新请求

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)

# 在settings文件中设置开启中间件

# selenium在scrapy中的使用流程
在爬虫类中定义一个bro的属性，就是实例化的浏览器对象
在爬虫类中重写父类的一个closed(self,spider)，在该方法中关闭bro
在中间件中进行浏览器自动化操作

# 图片懒加载
我们要基于伪属性去获取地址

# 对于图片类型的数据管道类有专门处理的方法
from scrapy.pipelines.images import ImagesPipeline
class MyImage(ImagesPipeline):
    # 该方法是用来对媒体资源进行请求的，item就是爬虫类提交的
    def get_media_requests(self,item,info):
        yield scrapy.Request(item['img_url'])
    # 指明数据的存储路径
    def file_path(self,request,response=None,info=None):
        return path
    def item_complated(self,results,item,info):
        return item
# 在settings里面设置
IMAGES_STORE='path' # 指定图片存储文件夹的路径
~~~

## CrawlSpider

~~~python
# 新建一个爬虫文件
scrapy genspider -t crawl 文件名 起始url
# 两个新功能
链接提取器：LinkExtractor
规则解析器：Rule

# crawlspider文件
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

# Rule方法无法用请求传参传递item
# 如果有需求，需要创建多个item
class QuanSpider(CrawlSpider):
    name = 'quan'
    # allowed_domains = ['www.xxxx.com']
    start_urls = ['http://www.xxxx.com/']
    # 实例化一个链接提取器对象
    # 作用是根据指定规则（allow='正则表达式'）进行指定连接的提取
    # 一个LinkExtractor对象，对应一个Rule
	link = LinkExtractor(allow=r'Items/')
    rules = (
        # 用于解析数据，follow为true则表示爬取全站的连接，为false表示指爬取当前页面
        # follow为true：将连接提取器继续作用到链接所对应的页面中
        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        item = {}
        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        #item['name'] = response.xpath('//div[@id="name"]').get()
        #item['description'] = response.xpath('//div[@id="description"]').get()
        return item

 # 如果有多个item提交给了管道类
item.__class__.__name__ 判断item是来自哪个类
~~~

## 分布式

~~~python
# 原生的scrapy框架是不可以实现分布式的，因为调度器不可以共享
# 实现分布式需要联合scrapy_redis实现分布式，这个组件的作用是提供可以被共享的调度器和管道

# 创建工程、爬虫文件
# 修改爬虫类
from scrapy_redis.spiders import RedisSpider # 相当于scrapy.Spider
from scrapy_redis.spiders import RedisCrawlSpider # 相当于CrawlSpider
# 修改当前爬虫类的继承父类
class QuanSpider(RedisCrawlSpider):
    name = 'quan'
    # allowed_domains = ['www.xxxx.com']
    # start_urls = ['http://www.xxxx.com/']
    # 注销起始url，并注册一个属性redis_key，表示可以被共享的调度器队列名
    redis_key = '共享调度器队列的名称'
	link = LinkExtractor(allow=r'Items/')
    rules = (

        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        return item

# 在settings文件中配置管道类
ITEM_PIPELINES = {
   'scrapy_redis.pipelines.RedisPipeline': 300,
}

# 指定调度器
# 增加一个去重容器类的配置，作为使用Redis的set集合来存储请求的指纹数据，从而实现请求去重的持久化，即去重队列
DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDuperFilter'
# 使用scrapy_redis组件的调度器
SCHEDULER='scrapy_redis.scheduler.Scheduler'
# 配置调度器是否持久化存储，也就是当前爬虫结束了要不要清空redis中的请求队列和去重指纹的set，True表示不清除
SCHEDULER_PERSIST = True

# 指定redis数据库
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
REDIS_ENCODING = "utf-8"

# REDIS_PARAMS  = {'password':'xxxx'}    
# Redis连接参数，默认：REDIS_PARAMS = {'socket_timeout': 30,'socket_connect_timeout': 30,'retry_on_timeout': True,'encoding': REDIS_ENCODING,}）

# REDIS_URL = 'redis://user:pass@hostname:6379' #连接URL（优先于以上配置）

# redis的配置文件进行配置redis.windows.conf
	关闭默认绑定：注销第56行的内容 bind 127.0.0.1
    关闭保护模式：第75行protected-mode，yes改为no
# 启动redis

# 启动程序
scrapy runspider xxx.py

# 向调度器队列中扔一个起始url，redis中执行
lpush 共享调度器队列名 url
~~~

## 增量式

~~~python
# 用于检测网站的数据更新情况
# 核心机制：去重
# 可以利用redis的set实现
# sadd 结构名 url ---- 如果数据重复则会返回0，那么就代表对该url发送过请求，于是我们就不再请求这个url

# 本质其实就是存在一张对发起过请求的url保存的表，然后再下一次爬取数据的时候对比这张表里面的url，如果存在就不再请求。
# 保存的形式可以是多样的，比如加密过的url
~~~



# 重点：xpath表达式内不能有tbody



# 反爬机制

